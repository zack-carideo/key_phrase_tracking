{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages for keyword extraction\n",
    "- gensim: A library for topic modeling, which can also be used for extracting key phrases using the TextRank algorithm.\n",
    "- nltk: The Natural Language Toolkit provides various tools and algorithms for natural language processing, including key phrase extraction.\n",
    "- spaCy: A powerful library for natural language processing, which includes functionality for extracting key phrases.\n",
    "- summa: A library specifically designed for text summarization, but it also includes a keyword extraction module that can be used for extracting key phrases.\n",
    "- pytextrank: A Python implementation of the TextRank algorithm, which can be used for key phrase extraction.\n",
    "- yake: \n",
    "- kpminer:  \n",
    "- pke: \n",
    "\n",
    "##### Unsupervised graph based Keyword Extraction Models \n",
    "- TextRank\n",
    "- SingleRank\n",
    "- TopicRank\n",
    "- TopicalPageRank\n",
    "- PositionRank\n",
    "- MultipartiteRank\n",
    "\n",
    "##### statistical keyword extraction models \n",
    "- TF-IDF\n",
    "- KPMiner\n",
    "- YAKE!\n",
    "\n",
    "##### data processing packages used. \n",
    "- pandas, polars , contractions, symspellpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zjc1002/envs/key_phrase_extraction/lib/python3.11/site-packages\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "from spello.model import SpellCorrectionModel\n",
    "import io, re, string, spello,requests , zipfile, os, nltk, spacy , pytextrank\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup # For removing HTML\n",
    "import contractions # For expanding contractions\n",
    "from unidecode import unidecode # For handling accented words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_unzip(url: str, destination_folder: str):\n",
    "    \n",
    "    \"\"\"\n",
    "    Downloads a file from the given URL and extracts its contents to the specified destination folder.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the file to download.\n",
    "        destination_folder (str): The path to the folder where the contents of the zip file will be extracted.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Send a GET request to download the file\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "\n",
    "        # Read the content of the response\n",
    "        content = response.content\n",
    "\n",
    "        # Create a file-like object from the response content\n",
    "        file = io.BytesIO(content)\n",
    "\n",
    "        # Extract the contents of the zip file\n",
    "        with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(destination_folder)\n",
    "    else:\n",
    "        print(\"Failed to download the file.\")\n",
    "\n",
    "def correct_spelling( model: SpellCorrectionModel,text: str) -> str:\n",
    "    \"\"\"\n",
    "    Corrects the spelling of the given text using the specified spell correction model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to correct.\n",
    "        model (SpellCorrectionModel): The spell correction model to use.\n",
    "\n",
    "    Returns:\n",
    "        str: The corrected text.\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Correct the spelling of the text\n",
    "    corrected_text = model.spell_correct(text)['spell_corrected_text']\n",
    "\n",
    "    return corrected_text\n",
    "\n",
    "def preprocess_text(text: str):\n",
    "    def remove_html(text):\n",
    "        soup = BeautifulSoup(text)\n",
    "        text = soup.get_text()\n",
    "        return text\n",
    "\n",
    "    def remove_urls(text):\n",
    "        pattern = re.compile(r'https?://(www\\.)?(\\w+)(\\.\\w+)(/\\w*)?')\n",
    "        text = re.sub(pattern, \"\", text)\n",
    "        return text\n",
    "\n",
    "    def remove_emails(text):\n",
    "        pattern = re.compile(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\")\n",
    "        text = re.sub(pattern, \"\", text)\n",
    "        return text\n",
    "\n",
    "    def handle_accents(text):\n",
    "        text = unidecode(text)\n",
    "        return text\n",
    "\n",
    "    def remove_unicode_chars(text):\n",
    "        text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "        return text\n",
    "\n",
    "    def remove_punctuations(text):\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), \" \",text)\n",
    "        return text\n",
    "\n",
    "    def remove_digits(text):\n",
    "        pattern = re.compile(\"\\w*\\d+\\w*\")\n",
    "        text = re.sub(pattern, \"\",text)\n",
    "        return text\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "        return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "    def remove_extra_spaces(text):\n",
    "        text = re.sub(' +', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    #return remove_html(contractions.fix(remove_urls(remove_emails(handle_accents(remove_unicode_chars(remove_punctuations(remove_digits(remove_stopwords(remove_extra_spaces(correct_spelling(sp_model,text)))))))))))\n",
    "    return remove_html(contractions.fix(remove_urls(remove_emails(handle_accents(remove_unicode_chars(remove_punctuations(remove_digits(remove_stopwords(remove_extra_spaces(text))))))))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg = {'data_path':'/home/zjc1002/Mounts/data/cfpb/cfpb_complaints.csv'\n",
    "       , 'incols':['Date received', 'Product','Consumer complaint narrative','Company public response']\n",
    "       , 'text_cols':['Consumer complaint narrative','Company public response']\n",
    "       , 'spell_correct_model_download_dir':\"/home/zjc1002/Mounts/temp/\"\n",
    "       , 'spell_correct_model_url': \"https://haptik-website-images.haptik.ai/spello_models/en_large.pkl.zip\"\n",
    "       }\n",
    "\n",
    "\n",
    "#manually download spacy model to disk for use in future \n",
    "cache_dir=\"/home/zjc1002/Mounts/temp/\"\n",
    "model_path=\"en_core_web_lg\"\n",
    "\n",
    "# URL of the file to download\n",
    "url = \"https://haptik-website-images.haptik.ai/spello_models/en_large.pkl.zip\"\n",
    "destination_folder = \"/home/zjc1002/Mounts/temp/\"\n",
    "\n",
    "incols = ['Date received', 'Product','Consumer complaint narrative','Company public response']\n",
    "text_cols = ['Consumer complaint narrative','Company public response']\n",
    "n_samp = 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spell check model folder exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/zjc1002/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#download spacy model \n",
    "if not os.path.exists(Path(cache_dir,model_path).as_posix()):\n",
    "    spacy.cli.download(model_path)\n",
    "\n",
    "#load spacy model \n",
    "nlp = spacy.load(model_path)\n",
    "nlp.to_disk(os.path.join(cache_dir,model_path))\n",
    "nlp = spacy.load(os.path.join(cache_dir,model_path))\n",
    "\n",
    "#download stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords # For removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#download the spell correction model (just copy in paste if behind firewall) \n",
    "if os.path.exists(destination_folder):\n",
    "    print(\"The spell check model folder exists.\")\n",
    "else:\n",
    "    download_and_unzip(url, destination_folder)\n",
    "\n",
    "#spell correction model, we dont use it \n",
    "#sp = SpellCorrectionModel(language='en')\n",
    "#sp.load(Path(destination_folder,url.split('/')[-1].replace('.zip','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌───────────────┬───────────────────────────────────┬───────────────────────────────────┐\n",
      "│ Date received ┆ Product                           ┆ input_col                         │\n",
      "│ ---           ┆ ---                               ┆ ---                               │\n",
      "│ str           ┆ str                               ┆ str                               │\n",
      "╞═══════════════╪═══════════════════════════════════╪═══════════════════════════════════╡\n",
      "│ 03/03/2018    ┆ Credit reporting, credit repair … ┆ On XX XX  item showed credit rep… │\n",
      "│ 01/02/2019    ┆ Debt collection                   ┆ Saw credit report collection    … │\n",
      "│ 01/08/2019    ┆ Credit reporting, credit repair … ┆ I public record   Child Support … │\n",
      "│ 12/19/2018    ┆ Credit reporting, credit repair … ┆ XXXX furnished ficticious deroga… │\n",
      "│ 07/09/2018    ┆ Mortgage                          ┆ Complaint SUNTRUST MORTGAGE rega… │\n",
      "└───────────────┴───────────────────────────────────┴───────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a polars DataFrame\n",
    "df = (pl.read_csv(cfg['data_path'], has_header=True)[incols]).drop_nulls(subset=text_cols)\n",
    "\n",
    "# Create a new column 'input_txt' by concatenating 'Consumer complaint narrative' and 'Company public response'\n",
    "df = (df.with_columns(pl.concat_str([pl.col('Consumer complaint narrative')\n",
    "                                    , pl.col('Company public response')]).alias('input_col'))\n",
    "                                    ).select(pl.col(\"*\").exclude(text_cols))\n",
    "\n",
    "\n",
    "#preprocess text\n",
    "#df = df.map_rows(lambda t: preprocess_text(t[2]))\n",
    "df = df.with_columns(pl.col(\"input_col\").map_elements(preprocess_text, return_dtype=str, strategy= 'thread_local'))\n",
    "\n",
    "#Show the resulting DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date received', 'Product', 'input_col', 'keywords']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Spacy (unigram monitoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>year_month</th><th>Product</th><th>keywords</th><th>count</th></tr><tr><td>str</td><td>str</td><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;2015/04&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;practices&quot;</td><td>2</td></tr><tr><td>&quot;2015/04&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;equity&quot;</td><td>2</td></tr><tr><td>&quot;2015/04&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;re&quot;</td><td>2</td></tr><tr><td>&quot;2015/04&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;WE&quot;</td><td>2</td></tr><tr><td>&quot;2015/04&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;applicatin&quot;</td><td>2</td></tr><tr><td>&quot;2015/04&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;call&quot;</td><td>2</td></tr><tr><td>&quot;2015/04&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;USBank&quot;</td><td>2</td></tr><tr><td>&quot;2015/04&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;loan&quot;</td><td>8</td></tr><tr><td>&quot;2015/04&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;Unfair&quot;</td><td>2</td></tr><tr><td>&quot;2015/04&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;House&quot;</td><td>2</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 4)\n",
       "┌────────────┬───────────────┬────────────┬───────┐\n",
       "│ year_month ┆ Product       ┆ keywords   ┆ count │\n",
       "│ ---        ┆ ---           ┆ ---        ┆ ---   │\n",
       "│ str        ┆ str           ┆ str        ┆ u32   │\n",
       "╞════════════╪═══════════════╪════════════╪═══════╡\n",
       "│ 2015/04    ┆ Consumer Loan ┆ practices  ┆ 2     │\n",
       "│ 2015/04    ┆ Consumer Loan ┆ equity     ┆ 2     │\n",
       "│ 2015/04    ┆ Consumer Loan ┆ re         ┆ 2     │\n",
       "│ 2015/04    ┆ Consumer Loan ┆ WE         ┆ 2     │\n",
       "│ 2015/04    ┆ Consumer Loan ┆ applicatin ┆ 2     │\n",
       "│ 2015/04    ┆ Consumer Loan ┆ call       ┆ 2     │\n",
       "│ 2015/04    ┆ Consumer Loan ┆ USBank     ┆ 2     │\n",
       "│ 2015/04    ┆ Consumer Loan ┆ loan       ┆ 8     │\n",
       "│ 2015/04    ┆ Consumer Loan ┆ Unfair     ┆ 2     │\n",
       "│ 2015/04    ┆ Consumer Loan ┆ House      ┆ 2     │\n",
       "└────────────┴───────────────┴────────────┴───────┘"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "# Customize your stopwords list\n",
    "stopwords.add('new_stopword'),\n",
    "stopwords |= {\"Afham\",\"Farden\"}\n",
    "\n",
    "stopwords.remove('new_stopword'),\n",
    "stopwords -= {\"Afham\",\"Farden\"}\n",
    "\n",
    "def get_keywords_using_spacy(text,    pos_tag = ['PROPN', 'ADJ', 'NOUN'] ):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Set the hot words as the words with pos tag “PROPN“, “ADJ“, or “NOUN“. (POS tag list is customizable)\n",
    "    keywords = ([token.text for token in doc if token not in stopwords if not token.is_punct if token.pos_ in pos_tag])\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "#idientify keywords per group\n",
    "df = df.sample(n_samp).with_columns(keywords = pl.col('input_col').map_elements(get_keywords_using_spacy, strategy= 'thread_local'))\n",
    "df = df.explode('keywords')\n",
    "\n",
    "#format dates \n",
    "df = df.with_columns(date_recieved = pl.col('Date received').str.to_datetime(\"%m/%d/%Y\"))\n",
    "df = df.with_columns(year_month=pl.col('date_recieved').dt.strftime(\"%Y/%m\"))\n",
    "\n",
    "#generate datafarme of ngrams by date and product to plot / derive monitoring rules to define EMERINGING\n",
    "plot_df = df.groupby(['year_month','Product','keywords']).count()\n",
    "plot_df = plot_df.sort(by=['year_month','Product'])\n",
    "\n",
    "#calculate %chagne in each group / ngram\n",
    "plot_df = plot_df.sort(['year_month', 'Product']).with_columns([pl.col('count').pct_change().over(['year_month','Product']).alias('pct_chg')])\n",
    "plot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (366, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>year_month</th><th>Product</th><th>keywords</th><th>count</th><th>pct_chg</th></tr><tr><td>str</td><td>str</td><td>str</td><td>u32</td><td>f64</td></tr></thead><tbody><tr><td>&quot;2015/04&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;practices&quot;</td><td>2</td><td>null</td></tr><tr><td>&quot;2015/04&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;equity&quot;</td><td>2</td><td>0.0</td></tr><tr><td>&quot;2015/04&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;re&quot;</td><td>2</td><td>0.0</td></tr><tr><td>&quot;2015/04&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;WE&quot;</td><td>2</td><td>0.0</td></tr><tr><td>&quot;2015/04&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;applicatin&quot;</td><td>2</td><td>0.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;2016/12&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;CFPB&quot;</td><td>8</td><td>1.0</td></tr><tr><td>&quot;2016/12&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;number&quot;</td><td>4</td><td>-0.5</td></tr><tr><td>&quot;2016/12&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;due&quot;</td><td>4</td><td>0.0</td></tr><tr><td>&quot;2016/12&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;settlement&quot;</td><td>4</td><td>0.0</td></tr><tr><td>&quot;2016/12&quot;</td><td>&quot;Consumer Loan&quot;</td><td>&quot;loan&quot;</td><td>12</td><td>2.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (366, 5)\n",
       "┌────────────┬───────────────┬────────────┬───────┬─────────┐\n",
       "│ year_month ┆ Product       ┆ keywords   ┆ count ┆ pct_chg │\n",
       "│ ---        ┆ ---           ┆ ---        ┆ ---   ┆ ---     │\n",
       "│ str        ┆ str           ┆ str        ┆ u32   ┆ f64     │\n",
       "╞════════════╪═══════════════╪════════════╪═══════╪═════════╡\n",
       "│ 2015/04    ┆ Consumer Loan ┆ practices  ┆ 2     ┆ null    │\n",
       "│ 2015/04    ┆ Consumer Loan ┆ equity     ┆ 2     ┆ 0.0     │\n",
       "│ 2015/04    ┆ Consumer Loan ┆ re         ┆ 2     ┆ 0.0     │\n",
       "│ 2015/04    ┆ Consumer Loan ┆ WE         ┆ 2     ┆ 0.0     │\n",
       "│ 2015/04    ┆ Consumer Loan ┆ applicatin ┆ 2     ┆ 0.0     │\n",
       "│ …          ┆ …             ┆ …          ┆ …     ┆ …       │\n",
       "│ 2016/12    ┆ Consumer Loan ┆ CFPB       ┆ 8     ┆ 1.0     │\n",
       "│ 2016/12    ┆ Consumer Loan ┆ number     ┆ 4     ┆ -0.5    │\n",
       "│ 2016/12    ┆ Consumer Loan ┆ due        ┆ 4     ┆ 0.0     │\n",
       "│ 2016/12    ┆ Consumer Loan ┆ settlement ┆ 4     ┆ 0.0     │\n",
       "│ 2016/12    ┆ Consumer Loan ┆ loan       ┆ 12    ┆ 2.0     │\n",
       "└────────────┴───────────────┴────────────┴───────┴─────────┘"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_df.filter((pl.col('Product')=='Consumer Loan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Option 2: PyTextRank\n",
    "\n",
    "#calculate standard deviation on frequency of each term by group\n",
    "# Group by 'year_month' and 'Product' and calculate the percent change in 'count'\n",
    "\n",
    "percent_change_df = plot_df.groupby(['year_month', 'Product']).pct_change('count').alias('percent_change')\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "print(percent_change_df)\n",
    "#failed attempt\n",
    "#aggregate text from each group into a singel string to identify keyphrases \n",
    "# group_text = (df.group_by('Product').agg(pl.col('input_col').alias('product_text'))\n",
    "#               ).with_columns(final_text = pl.col('product_text').list.join('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: PyTextRank"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
